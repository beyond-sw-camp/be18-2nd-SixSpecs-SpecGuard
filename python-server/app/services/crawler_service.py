import os
from typing import Optional, List, Dict, Any
from fastapi import HTTPException

from app.crawlers import velog_crawler as vc
from app.utils.dates import normalize_created_at
from app.services.nlp_client import send_to_nlp
from app.services.storage_client import send_to_storage


def _env_bool(key: str, default: str = "false") -> bool:
    return os.environ.get(key, default).lower() in {"1", "true", "yes"}


def _env_int(key: str, default: str) -> int:
    try:
        return int(os.environ.get(key, default))
    except Exception:
        return int(default)


# ëª©ë¡
async def list_posts(username: str, page: int, limit: int):
    links: List[str] = await vc.render_list_with_playwright(username)
    if not links:
        return []

    start, end = (page - 1) * limit, (page - 1) * limit + limit
    out = []
    for url in links[start:end]:
        title, _text, _langs, tags, published = await vc.render_post_with_playwright(url)
        out.append(
            {
                "title": title or "",
                "url": url,
                "date": normalize_created_at(published),
                "tags": tags or [],
            }
        )
    return out


# ìƒì„¸
async def post_detail(url: str):
    title, text, langs, tags, published = await vc.render_post_with_playwright(url)

    # ğŸ‘‰ í¬ë¡¤/íŒŒì‹± ì‹¤íŒ¨ë¡œ íŒë‹¨(ì •ì±…: ì œëª©/ë³¸ë¬¸ ëª¨ë‘ ì—†ìŒ)
    if not title and not text:
        raise HTTPException(
            status_code=500,
            detail={
                "error": "CRAWLING_FAILED",
                "message": "Velog êµ¬ì¡° ë³€ê²½ ë˜ëŠ” ì˜ˆì™¸ë¡œ ì¸í•´ ê²Œì‹œê¸€ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            },
        )

    MAX_TEXT_LEN = _env_int("MAX_TEXT_LEN", "200000")
    if text and len(text) > MAX_TEXT_LEN:
        text = text[:MAX_TEXT_LEN]

    return {
        "title": title,
        "url": url,
        "createdAt": normalize_created_at(published),
        "content": text or "",
        "tags": tags or [],
        "codeLangs": langs or [],
    }


# NLPë¡œ ì „ì²´/ì¼ë¶€ ì „ì†¡
async def crawl_and_forward(username: str, nlp_url: str, body_max_posts: Optional[int]):
    data = await vc.crawl_all_posts(username)  # (limit_posts ì“°ëŠ” ë²„ì „ì´ë©´ ì ìš©)

    if not data.get("posts"):
        raise HTTPException(
            status_code=500,
            detail={
                "error": "CRAWLING_FAILED",
                "message": "ì „ë‹¬í•  ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš©ì ë˜ëŠ” Velog êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”"
            },
        )

    nlp_resp = await send_to_nlp(nlp_url, data)
    return len(data["posts"]), nlp_resp


async def crawl_and_store(username: str, resume_id: str, storage_url: str, body_max_posts: Optional[int]):
    data = await vc.crawl_all_posts(username)  # (limit_posts ì“°ëŠ” ë²„ì „ì´ë©´ ì ìš©)


    if not data.get("posts"):
        raise HTTPException(
            status_code=500,
            detail={
                "error": "CRAWLING_FAILED",
                "message": "ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤. í•¸ë“¤ì´ ì˜¬ë°”ë¥¸ì§€ ë˜ëŠ” Velog êµ¬ì¡° ë³€ê²½ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”"
            },
        )

    payload = [
        {
            "url": p["url"],
            "link_type": "velog",
            "contents": {
                "title": p["title"], "text": p["text"], "tags": p["tags"],
                "code_langs": p["code_langs"], "published_at": p["published_at"],
                "content_hash": p["content_hash"], "source": "velog",
            },
            "resume_id": resume_id,
        }
        for p in data["posts"]
    ]

    storage_resp = await send_to_storage(storage_url, payload)
    return {"count": len(payload), "storage_response": storage_resp}
